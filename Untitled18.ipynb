{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8c4qx95ZV5IMvFRIiuKlb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshulchauhan502/btp/blob/main/Untitled18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnlMGOh-WPoO",
        "outputId": "5ee84dff-0de4-477c-d9c3-a049a051580c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flwr tensorflow numpy pandas scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aFRTEtJmWagU",
        "outputId": "209ea744-dc3d-4e3d-ffd5-4828f00310f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr\n",
            "  Downloading flwr-1.22.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting click<8.2.0 (from flwr)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting cryptography<45.0.0,>=44.0.1 (from flwr)\n",
            "  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr) (1.75.1)\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.62.3 (from flwr)\n",
            "  Downloading grpcio_health_checking-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.6 (from flwr)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr) (6.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr) (2.32.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.12/dist-packages (from flwr) (13.9.4)\n",
            "Collecting tomli<3.0.0,>=2.0.1 (from flwr)\n",
            "  Downloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (2.0.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-health-checking to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.62.3 (from flwr)\n",
            "  Downloading grpcio_health_checking-1.75.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.74.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.73.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.73.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.72.2-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.72.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.71.2-py3-none-any.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-health-checking to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_health_checking-1.71.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_health_checking-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Downloading flwr-1.22.0-py3-none-any.whl (703 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m703.2/703.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_health_checking-1.62.3-py3-none-any.whl (18 kB)\n",
            "Downloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tomli-w, tomli, pycryptodome, protobuf, pathspec, iterators, click, grpcio-health-checking, cryptography, typer, flwr\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.19.2\n",
            "    Uninstalling typer-0.19.2:\n",
            "      Successfully uninstalled typer-0.19.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.1.8 cryptography-44.0.3 flwr-1.22.0 grpcio-health-checking-1.62.3 iterators-0.0.2 pathspec-0.12.1 protobuf-4.25.8 pycryptodome-3.23.0 tomli-2.2.1 tomli-w-1.2.0 typer-0.12.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4a33dc391fc84c0eabc0c4abbbda019c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STAGE 2: Federated (IID) - FedAvg simulation (EfficientNetB0)\n",
        "# Self-contained; reuses same preprocessing/model as Stage-1\n",
        "# ================================\n",
        "\n",
        "# ---------- Cell A: Installs (uncomment if flwr is required later) ----------\n",
        "# !pip install -q flwr    # not required for this script (we use a simple FedAvg sim)\n",
        "\n",
        "# ---------- Cell 1: Imports, setup ----------\n",
        "import os, random, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision, callbacks, optimizers\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Repro\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Mixed precision (same as Stage-1)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
        "\n",
        "# Paths (update if different)\n",
        "DATA_DIR = \"/content/drive/MyDrive/ham10000_data\"\n",
        "IMG_DIR = os.path.join(DATA_DIR, \"all_images\")\n",
        "META_CSV = os.path.join(DATA_DIR, \"HAM10000_metadata.csv\")\n",
        "\n",
        "# ---------- Config (tune these) ----------\n",
        "NUM_CLIENTS = 5          # number of simulated hospitals\n",
        "ROUNDS = 8               # federated rounds (global aggregation steps)\n",
        "LOCAL_EPOCHS = 1         # local training epochs per round\n",
        "CLIENT_BATCH_SIZE = 32   # smaller than centralized to save memory\n",
        "IMG_SIZE = 224\n",
        "VERBOSE = 1\n",
        "\n",
        "# ---------- Utility ----------\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# ---------- Cell 2: Load metadata and build image_path, filter missing ----------\n",
        "meta = pd.read_csv(META_CSV, dtype=str)\n",
        "\n",
        "# ensure image_id exists\n",
        "if 'image_id' not in meta.columns and 'imageId' in meta.columns:\n",
        "    meta = meta.rename(columns={'imageId':'image_id'})\n",
        "\n",
        "meta['image_id'] = meta['image_id'].astype(str)\n",
        "# create a case-insensitive mapping from file stems to actual file path\n",
        "existing_files = list(Path(IMG_DIR).glob(\"*\"))\n",
        "stem2path = {p.stem.lower(): str(p) for p in existing_files}\n",
        "\n",
        "def get_path(img_id):\n",
        "    return stem2path.get(img_id.lower(), None)\n",
        "\n",
        "meta['image_path'] = meta['image_id'].apply(get_path)\n",
        "missing = meta['image_path'].isna().sum()\n",
        "print(\"Metadata rows:\", len(meta), \"Missing images:\", missing)\n",
        "meta = meta.dropna(subset=['image_path']).reset_index(drop=True)\n",
        "print(\"After filtering, rows:\", len(meta))\n",
        "\n",
        "# ---------- Cell 3: Binary label mapping and Train/Val/Test split ----------\n",
        "benign_labels = [\"nv\", \"bkl\", \"df\", \"vasc\"]\n",
        "malignant_labels = [\"mel\", \"bcc\", \"akiec\"]\n",
        "\n",
        "meta['binary_label'] = meta['dx'].apply(lambda x: 1 if x in malignant_labels else 0).astype(int)\n",
        "print(\"Overall class counts:\\n\", meta['binary_label'].value_counts())\n",
        "\n",
        "# Centralized splits (to keep identical to Stage-1)\n",
        "train_df, temp_df = train_test_split(meta, test_size=0.3, stratify=meta['binary_label'], random_state=SEED)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.33, stratify=temp_df['binary_label'], random_state=SEED)\n",
        "print(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "\n",
        "# Compute global class-weights (used for local training to handle imbalance)\n",
        "classes = np.unique(train_df['binary_label'])\n",
        "weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['binary_label'])\n",
        "class_weights = {int(c): float(w) for c, w in zip(classes, weights)}\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# ---------- Cell 4: IID client split (label-balanced) ----------\n",
        "def split_iid_labelwise(df, n_clients):\n",
        "    \"\"\"Split df into n_clients IID by distributing each label's indices equally.\"\"\"\n",
        "    clients = [pd.DataFrame(columns=df.columns) for _ in range(n_clients)]\n",
        "    for label in sorted(df['binary_label'].unique()):\n",
        "        label_df = df[df['binary_label'] == label].sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "        parts = np.array_split(label_df, n_clients)\n",
        "        for i, part in enumerate(parts):\n",
        "            clients[i] = pd.concat([clients[i], part], ignore_index=True)\n",
        "    # shuffle rows inside each client\n",
        "    for i in range(n_clients):\n",
        "        clients[i] = clients[i].sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "    return clients\n",
        "\n",
        "client_dfs = split_iid_labelwise(train_df, NUM_CLIENTS)\n",
        "for i, cdf in enumerate(client_dfs):\n",
        "    print(f\"Client {i}: {len(cdf)} samples, label distribution:\\n{cdf['binary_label'].value_counts().to_dict()}\")\n",
        "\n",
        "# ---------- Cell 5: Dataset builder (same preprocessing as Stage-1) ----------\n",
        "# Preprocess function (pure TF graph)\n",
        "def preprocess_tf(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = preprocess_input(img)   # EfficientNet preprocessing\n",
        "    return img, label\n",
        "\n",
        "# Simple augmentation (same as Stage-1)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomRotation(0.06),\n",
        "    tf.keras.layers.RandomZoom(0.08),\n",
        "    tf.keras.layers.RandomTranslation(0.05, 0.05),\n",
        "])\n",
        "\n",
        "def make_dataset_from_df(df, batch_size=CLIENT_BATCH_SIZE, training=False):\n",
        "    paths = df['image_path'].values\n",
        "    labels = df['binary_label'].astype(np.int32).values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    ds = ds.map(preprocess_tf, num_parallel_calls=AUTOTUNE)\n",
        "    if training:\n",
        "        ds = ds.shuffle(1000, seed=SEED)\n",
        "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Build client datasets (train and small local val)\n",
        "client_train_ds = []\n",
        "client_val_ds = []\n",
        "client_num_examples = []\n",
        "for cdf in client_dfs:\n",
        "    # small local validation split per client (10% holdout)\n",
        "    if len(cdf) > 10:\n",
        "        local_train, local_val = train_test_split(cdf, test_size=0.10, stratify=cdf['binary_label'], random_state=SEED)\n",
        "    else:\n",
        "        local_train, local_val = cdf, cdf  # tiny clients - use same for val\n",
        "    client_train_ds.append(make_dataset_from_df(local_train, batch_size=CLIENT_BATCH_SIZE, training=True))\n",
        "    client_val_ds.append(make_dataset_from_df(local_val, batch_size=CLIENT_BATCH_SIZE, training=False))\n",
        "    client_num_examples.append(len(local_train))\n",
        "\n",
        "# Centralized test dataset (for final evaluation)\n",
        "test_ds = make_dataset_from_df(test_df, batch_size=CLIENT_BATCH_SIZE, training=False)\n",
        "\n",
        "# ---------- Cell 6: Model builder (returns model and base model) ----------\n",
        "def build_model(base_trainable=True):\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
        "    base_model.trainable = base_trainable\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    out = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=out)\n",
        "    return model, base_model\n",
        "\n",
        "# ---------- Cell 7: Create client models (one per client) ----------\n",
        "client_models = []\n",
        "for i in range(NUM_CLIENTS):\n",
        "    m, b = build_model(base_trainable=True)   # allow base to be trainable for federated updates\n",
        "    m.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    client_models.append((m, b))\n",
        "\n",
        "# Create a global model (will hold aggregated weights)\n",
        "global_model, _ = build_model(base_trainable=True)\n",
        "global_model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "# Initialize global weights from centralized pretrained head (or ImageNet init)\n",
        "global_weights = global_model.get_weights()\n",
        "\n",
        "# ---------- Cell 8: FedAvg simulation ----------\n",
        "def fedavg(weights_list, weights_counts):\n",
        "    \"\"\"Weighted average of a list of weight lists (lists of ndarrays).\"\"\"\n",
        "    # weights_list: list of client weights (each is a list of numpy arrays)\n",
        "    # weights_counts: list of ints (num examples for each client)\n",
        "    total = float(sum(weights_counts))\n",
        "    new_weights = []\n",
        "    for layer_idx in range(len(weights_list[0])):\n",
        "        layer_sum = np.zeros_like(weights_list[0][layer_idx], dtype=np.float64)\n",
        "        for w, n in zip(weights_list, weights_counts):\n",
        "            layer_sum += (w[layer_idx].astype(np.float64) * (n / total))\n",
        "        new_weights.append(layer_sum.astype(weights_list[0][layer_idx].dtype))\n",
        "    return new_weights\n",
        "\n",
        "history = {\"round\": [], \"val_loss\": [], \"val_accuracy\": [], \"val_auc\": []}\n",
        "\n",
        "print(\"Starting FedAvg simulation: clients:\", NUM_CLIENTS, \"rounds:\", ROUNDS)\n",
        "for r in range(1, ROUNDS+1):\n",
        "    print(f\"\\n--- Round {r}/{ROUNDS} ---\")\n",
        "    client_weights = []\n",
        "    client_sizes = []\n",
        "    # For each client: set global weights, train locally, collect weights\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        model_i, _ = client_models[i]\n",
        "        # set global weights\n",
        "        model_i.set_weights(global_weights)\n",
        "        # local training\n",
        "        model_i.fit(client_train_ds[i],\n",
        "                    epochs=LOCAL_EPOCHS,\n",
        "                    class_weight=class_weights,\n",
        "                    verbose=VERBOSE)\n",
        "        # collect weights and size\n",
        "        client_weights.append([w.copy() for w in model_i.get_weights()])\n",
        "        client_sizes.append(client_num_examples[i])\n",
        "    # Aggregate weights (FedAvg)\n",
        "    global_weights = fedavg(client_weights, client_sizes)\n",
        "    global_model.set_weights(global_weights)\n",
        "    # evaluate global model on validation set (or test set)\n",
        "    val_loss, val_acc, val_auc = global_model.evaluate(make_dataset_from_df(val_df, batch_size=CLIENT_BATCH_SIZE, training=False), verbose=0)\n",
        "    print(f\"After round {r}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_auc={val_auc:.4f}\")\n",
        "    history[\"round\"].append(r)\n",
        "    history[\"val_loss\"].append(float(val_loss))\n",
        "    history[\"val_accuracy\"].append(float(val_acc))\n",
        "    history[\"val_auc\"].append(float(val_auc))\n",
        "\n",
        "# ---------- Cell 9: Final evaluation on centralized test set ----------\n",
        "test_loss, test_acc, test_auc = global_model.evaluate(test_ds, verbose=1)\n",
        "print(f\"\\nFinal global model on centralized test set -> Test Accuracy: {test_acc*100:.2f}%, Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Save final weights if needed\n",
        "out_dir = os.path.join(DATA_DIR, \"federated_iid_experiment\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "global_model.save(os.path.join(out_dir, \"global_model_fed_iid\"), include_optimizer=False)\n",
        "print(\"Saved global model to:\", os.path.join(out_dir, \"global_model_fed_iid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHgXtJxyWTtg",
        "outputId": "38c4263f-abe2-48a3-ed06-1faafe370527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mixed precision policy: <DTypePolicy \"mixed_float16\">\n",
            "Metadata rows: 10015 Missing images: 0\n",
            "After filtering, rows: 10015\n",
            "Overall class counts:\n",
            " binary_label\n",
            "0    8061\n",
            "1    1954\n",
            "Name: count, dtype: int64\n",
            "Train/Val/Test sizes: 7010 2013 992\n",
            "Class weights: {0: 0.6212336051045728, 1: 2.5621345029239766}\n",
            "Client 0: 1403 samples, label distribution:\n",
            "{0: 1129, 1: 274}\n",
            "Client 1: 1403 samples, label distribution:\n",
            "{0: 1129, 1: 274}\n",
            "Client 2: 1402 samples, label distribution:\n",
            "{0: 1128, 1: 274}\n",
            "Client 3: 1401 samples, label distribution:\n",
            "{0: 1128, 1: 273}\n",
            "Client 4: 1401 samples, label distribution:\n",
            "{0: 1128, 1: 273}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Starting FedAvg simulation: clients: 5 rounds: 8\n",
            "\n",
            "--- Round 1/8 ---\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6551s\u001b[0m 155s/step - accuracy: 0.5893 - auc: 0.6442 - loss: 0.6793\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6302s\u001b[0m 155s/step - accuracy: 0.5820 - auc: 0.6335 - loss: 0.6826\n",
            "\u001b[1m 3/40\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37:15\u001b[0m 158s/step - accuracy: 0.5972 - auc: 0.4888 - loss: 0.7476"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P9witNnlWejp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}